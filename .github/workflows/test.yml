name: Test Suite

on:
  push:
    branches: [main, develop]
    paths:
      - 'build-tools/**'
      - 'tests/**'
      - 'package.json'
      - 'jest.config.js'
  pull_request:
    branches: [main]
    paths:
      - 'build-tools/**'
      - 'tests/**'
      - 'package.json'
      - 'jest.config.js'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18.x, 20.x, 22.x]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm test

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: test-results-${{ matrix.node-version }}
          path: |
            coverage/
            *.log
          retention-days: 3

  coverage:
    name: Test Coverage
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests with coverage
        run: npm run test:coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

      - name: Upload coverage artifacts
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report
          path: coverage/
          retention-days: 7

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup Python for font processing
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install fonttools[ufo,lxml,woff,unicode]

      - name: Create font-tools-env symlink
        run: |
          python -m venv font-tools-env
          source font-tools-env/bin/activate
          pip install fonttools[ufo,lxml,woff,unicode]

      - name: Run integration tests
        run: npm test -- --testPathPattern="integration"
        continue-on-error: true

      - name: Test build pipeline integration
        run: |
          npm run scan || echo "Scanner test completed"
          npm run generate-modules || echo "Module generation test completed"
          npm run validate || echo "Validation test completed"

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run performance tests
        run: npm test -- --testPathPattern="performance" --verbose
        continue-on-error: true

      - name: Benchmark build performance
        run: |
          echo "🚀 Testing build performance..."
          time npm run build || echo "Build completed with status: $?"

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, coverage, integration-tests, performance-tests]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test artifacts
        uses: actions/download-artifact@v5
        continue-on-error: true

      - name: Generate test summary
        run: |
          echo "## 🧪 Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "✅ **Unit Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Unit Tests**: Failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.coverage.result }}" == "success" ]; then
            echo "✅ **Coverage**: Generated" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Coverage**: Failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.integration-tests.result }}" == "success" ]; then
            echo "✅ **Integration Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Integration Tests**: Issues detected" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance-tests.result }}" == "success" ]; then
            echo "✅ **Performance Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Performance Tests**: Issues detected" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📊 **Coverage Report**: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "🔍 **Detailed Results**: Check individual job logs" >> $GITHUB_STEP_SUMMARY